// bubble_sort.c
#include <stdio.h>
#include <stdlib.h>
#include <omp.h>
#include <time.h>

#define MAX 100000

void generateRandomArray(int *arr, int n) {
    for (int i = 0; i < n; i++)
        arr[i] = rand() % 10000;
}

void bubbleSortSeq(int *arr, int n) {
    for (int i = 0; i < n - 1; i++)
        for (int j = 0; j < n - i - 1; j++)
            if (arr[j] > arr[j + 1]) {
                int tmp = arr[j];
                arr[j] = arr[j + 1];
                arr[j + 1] = tmp;
            }
}

void bubbleSortPar(int *arr, int n) {
    for (int i = 0; i < n - 1; i++) {
        int start = i % 2;
        #pragma omp parallel for
        for (int j = start; j < n - 1; j += 2) {
            if (arr[j] > arr[j + 1]) {
                int tmp = arr[j];
                arr[j] = arr[j + 1];
                arr[j + 1] = tmp;
            }
        }
    }
}

void printArray(int *arr, int n) {
    for (int i = 0; i < n && i < 20; i++) {
        printf("%5d ", arr[i]);
    }
    if (n > 20) printf("...");  // Avoid printing giant arrays
    printf("\n");
}

int main() {
    int n;
    printf("Enter number of elements: ");
    scanf("%d", &n);
    int *arr1 = malloc(n * sizeof(int));
    int *arr2 = malloc(n * sizeof(int));
    srand(time(NULL));
    generateRandomArray(arr1, n);
    for (int i = 0; i < n; i++) arr2[i] = arr1[i];

    printf("\nOriginal array:\n");
    printArray(arr1, n);

    double start, end;

    start = omp_get_wtime();
    bubbleSortSeq(arr1, n);
    end = omp_get_wtime();
    double seqTime = end - start;

    start = omp_get_wtime();
    bubbleSortPar(arr2, n);
    end = omp_get_wtime();
    double parTime = end - start;

    printf("\nSorted (Sequential):\n");
    printArray(arr1, n);
    printf("Time (Sequential Bubble Sort): %.6f s\n", seqTime);

    printf("\nSorted (Parallel):\n");
    printArray(arr2, n);
    printf("Time (Parallel Bubble Sort):   %.6f s\n", parTime);

    printf("\n%-25s %.2fx speedup\n", "Speedup (Par / Seq):", seqTime / parTime);

    free(arr1);
    free(arr2);
    return 0;
}
===============================================================================


merge sort:
// merge_sort_simplified.c
#include <stdio.h>
#include <stdlib.h>
#include <omp.h>    // Include OpenMP header
#include <time.h>   // For timing and seeding random numbers

// --- The Merge Function ---
// Merges two **already sorted** subarrays back into the main array.
// Subarray 1: arr[startIndex] ... arr[midIndex]
// Subarray 2: arr[midIndex + 1] ... arr[endIndex]
void merge(int *arrayToSort, int startIndex, int midIndex, int endIndex) {
    // Calculate sizes of the two temporary subarrays
    int sizeLeft = midIndex - startIndex + 1;
    int sizeRight = endIndex - midIndex;

    // Create temporary arrays to hold the left and right halves
    int *leftHalf = (int *)malloc(sizeLeft * sizeof(int));
    int *rightHalf = (int *)malloc(sizeRight * sizeof(int));

    // --- Copy data to temporary arrays ---
    // Copy elements from the main array into the left temporary array
    for (int i = 0; i < sizeLeft; i++) {
        leftHalf[i] = arrayToSort[startIndex + i];
    }
    // Copy elements from the main array into the right temporary array
    for (int j = 0; j < sizeRight; j++) {
        rightHalf[j] = arrayToSort[midIndex + 1 + j];
    }

    // --- Merge the temporary arrays back into the main array ---
    int indexLeft = 0;    // Initial index for the left temporary array
    int indexRight = 0;   // Initial index for the right temporary array
    int indexMerged = startIndex; // Initial index for the main array where merging starts

    // Compare elements from leftHalf and rightHalf and place the smaller one
    // into the main array. Continue until one of the temporary arrays is empty.
    while (indexLeft < sizeLeft && indexRight < sizeRight) {
        if (leftHalf[indexLeft] <= rightHalf[indexRight]) {
            arrayToSort[indexMerged] = leftHalf[indexLeft];
            indexLeft++;
        } else {
            arrayToSort[indexMerged] = rightHalf[indexRight];
            indexRight++;
        }
        indexMerged++;
    }

    // --- Copy any remaining elements ---
    // If there are elements left in leftHalf, copy them
    while (indexLeft < sizeLeft) {
        arrayToSort[indexMerged] = leftHalf[indexLeft];
        indexLeft++;
        indexMerged++;
    }
    // If there are elements left in rightHalf, copy them
    while (indexRight < sizeRight) {
        arrayToSort[indexMerged] = rightHalf[indexRight];
        indexRight++;
        indexMerged++;
    }

    // Free the memory allocated for the temporary arrays
    free(leftHalf);
    free(rightHalf);
}

// --- Sequential Merge Sort ---
// Standard recursive merge sort algorithm.
void sequentialMergeSort(int *arrayToSort, int startIndex, int endIndex) {
    // Base case: If the segment has 0 or 1 elements, it's already sorted.
    if (startIndex < endIndex) {
        // Find the middle point to divide the array
        int midIndex = startIndex + (endIndex - startIndex) / 2; // Avoids overflow for large indices

        // Recursively sort the first half
        sequentialMergeSort(arrayToSort, startIndex, midIndex);
        // Recursively sort the second half
        sequentialMergeSort(arrayToSort, midIndex + 1, endIndex);

        // Merge the two sorted halves
        merge(arrayToSort, startIndex, midIndex, endIndex);
    }
}

// --- Parallel Merge Sort using OpenMP Tasks ---

// Threshold: If the subarray size is below this, use sequential sort.
// This avoids the overhead of creating very small parallel tasks.
#define MIN_SIZE_FOR_PARALLEL 5000

void parallelMergeSort(int *arrayToSort, int startIndex, int endIndex) {
    // Check if the current segment is large enough for parallel execution
    if (startIndex < endIndex) {
        if ((endIndex - startIndex + 1) < MIN_SIZE_FOR_PARALLEL) {
            // --- Fallback Case ---
            // If the segment is too small, sort it sequentially.
            sequentialMergeSort(arrayToSort, startIndex, endIndex);
        } else {
            // --- Parallel Case ---
            // Find the middle point
            int midIndex = startIndex + (endIndex - startIndex) / 2;

            // *** Create Parallel Tasks ***
            // These tasks are like "job descriptions" added to a pool.
            // Available threads in the OpenMP team will pick up these tasks.

            #pragma omp task // Define the first task: sort the left half
            {
                // This block will be executed as an independent task
                parallelMergeSort(arrayToSort, startIndex, midIndex);
            } // Task for left half ends here

            #pragma omp task // Define the second task: sort the right half
            {
                // This block will also be executed as an independent task
                // Potentially at the same time as the left half task on another core
                parallelMergeSort(arrayToSort, midIndex + 1, endIndex);
            } // Task for right half ends here

            // *** Synchronization Point ***
            // #pragma omp taskwait: IMPORTANT!
            // The current thread will PAUSE here until BOTH of the child tasks
            // created *directly within this block* (sorting left and right halves)
            // have fully completed. We NEED both halves sorted before we can merge.
            #pragma omp taskwait

            // --- Merge Step ---
            // Only after both halves are confirmed sorted (due to taskwait),
            // merge them together. This merge is done by the current thread.
            merge(arrayToSort, startIndex, midIndex, endIndex);
        }
    }
}


// --- Utility Functions ---

// Fills an array with random integers.
void generateRandomArray(int *arr, int n) {
    for (int i = 1; i < n; i++) arr[i] = rand() % 10000000; // Numbers between 0 and 9999999
}

// Prints the first few elements of an array.
void printArray(int *arr, int n) {
    int limit = (n < 20) ? n : 20; // Print max 20 elements
    for (int i = 0; i < limit; i++)
        printf("%5d ", arr[i]);
    if (n > 20) printf("...");
    printf("\n");
}

// --- Main Function ---
int main() {
    int n; // Number of elements
    printf("Enter number of elements: ");
    scanf("%d", &n);

    // Allocate memory for two copies of the array
    int *arraySequential = (int *)malloc(n * sizeof(int));
    int *arrayParallel = (int *)malloc(n * sizeof(int));
    if (!arraySequential || !arrayParallel) {
        perror("Failed to allocate memory");
        return 1;
    }

    // Seed the random number generator
    srand(time(NULL));
    generateRandomArray(arraySequential, n);

    // Copy the unsorted data to the second array for parallel sorting
    for (int i = 0; i < n; i++) {
        arrayParallel[i] = arraySequential[i];
    }

    printf("\nOriginal array:\n");
    printArray(arraySequential, n);

    // --- Time and Run Sequential Sort ---
    printf("\nRunning Sequential Merge Sort...\n");
    double startTimeSeq = omp_get_wtime(); // Get start time
    sequentialMergeSort(arraySequential, 0, n - 1);
    double endTimeSeq = omp_get_wtime();   // Get end time
    double timeSequential = endTimeSeq - startTimeSeq;

    // --- Time and Run Parallel Sort ---
    printf("Running Parallel Merge Sort...\n");
    double startTimePar = omp_get_wtime(); // Get start time

    // *** Setting up the Parallel Region for Tasks ***
    // We need to create a team of threads *once* outside the recursive function.
    // The `#pragma omp parallel` directive creates the team.
    // The `#pragma omp single` directive ensures that ONLY ONE thread
    // from the team makes the *initial* call to parallelMergeSort.
    // That single call will then generate all the necessary tasks for the team to execute.
  
      // *** Optimization: Check if parallel region is even needed ***
    // If the total array size is smaller than the threshold where
    // parallelMergeSort would just call sequentialMergeSort anyway,
    // skip creating the parallel region to avoid its overhead.
    if (n > MIN_SIZE_FOR_PARALLEL) {
        // --- Use Parallel Execution ---
        #pragma omp parallel // Create the thread team
        {
            #pragma omp single // Only one thread starts the recursive task generation
            {
                parallelMergeSort(arrayParallel, 0, n - 1);
            } // End of single region
        } // End of parallel region (threads synchronize and disband here)
    } else {
        // --- Fallback for Small Arrays ---
        // The array is too small, just run the sequential version directly.
        // (Calling parallelMergeSort here would also work, as it would
        // immediately call sequentialMergeSort, but calling sequential directly
        // slightly more clearly shows what's happening).
        printf(" (Array size <= threshold, running sequentially)\n");
        sequentialMergeSort(arrayParallel, 0, n - 1);
        // Alternatively, you could still call parallelMergeSort here, it would just fall back:
        // parallelMergeSort(arrayParallel, 0, n - 1);
    }
    double endTimePar = omp_get_wtime();   // Get end time
    double timeParallel = endTimePar - startTimePar;

    // --- Print Results ---
    printf("\nSorted (Sequential):\n");
    printArray(arraySequential, n);
    printf("Time (Sequential Merge Sort): %.6f s\n", timeSequential);

    printf("\nSorted (Parallel):\n");
    printArray(arrayParallel, n);
    printf("Time (Parallel Merge Sort):   %.6f s\n", timeParallel);

    // Calculate and print speedup
    if (timeParallel > 1e-9 && timeSequential > 1e-9) { // Avoid division by zero or tiny numbers
         printf("\n%-25s %.2fx speedup\n", "Speedup (Par / Seq):", timeSequential / timeParallel);
    } else {
         printf("\nSpeedup calculation skipped due to very small timings.\n");
    }


    // --- Cleanup ---
    free(arraySequential);
    free(arrayParallel);
    return 0;
}




======================================================================================


MERGE-BUBBLE SORT
#include <stdio.h>
#include <stdlib.h>
#include <omp.h>
#include <time.h>

void insertionSort(int arr[], int n) {
    for (int i = 1; i < n; i++) {
        int key = arr[i];
        int j = i - 1;
        while (j >= 0 && arr[j] > key) {
            arr[j + 1] = arr[j];
            j--;
        }
        arr[j + 1] = key;
    }
}

void merge(int arr[], int left, int mid, int right) {
    int n1 = mid - left + 1;
    int n2 = right - mid;
    int *L = (int *)malloc(n1 * sizeof(int));
    int *R = (int *)malloc(n2 * sizeof(int));

    for (int i = 0; i < n1; i++)
        L[i] = arr[left + i];
    for (int j = 0; j < n2; j++)
        R[j] = arr[mid + 1 + j];

    int i = 0, j = 0, k = left;
    while (i < n1 && j < n2) {
        if (L[i] <= R[j]) {
            arr[k] = L[i];
            i++;
        } else {
            arr[k] = R[j];
            j++;
        }
        k++;
    }

    while (i < n1) {
        arr[k] = L[i];
        i++;
        k++;
    }

    while (j < n2) {
        arr[k] = R[j];
        j++;
        k++;
    }

    free(L);
    free(R);
}

void mergeSortSequential(int arr[], int left, int right) {
    if (left < right) {
        int mid = left + (right - left) / 2;
        mergeSortSequential(arr, left, mid);
        mergeSortSequential(arr, mid + 1, right);
        merge(arr, left, mid, right);
    }
}

void mergeSortParallel(int arr[], int left, int right) {
    if (left < right) {
        int mid = left + (right - left) / 2;

        #pragma omp parallel sections
        {
            #pragma omp section
            mergeSortParallel(arr, left, mid);
            
            #pragma omp section
            mergeSortParallel(arr, mid + 1, right);
        }

        merge(arr, left, mid, right);
    }
}

void bubbleSortOddEvenParallel(int arr[], int n) {
    int temp;

    // Parallelize Odd Phase
    #pragma omp parallel for private(temp)
    for (int i = 0; i < n / 2; i++) {
        for (int j = 1 + i; j < n - i - 1; j += 2) {
            if (arr[j] > arr[j + 1]) {
                temp = arr[j];
                arr[j] = arr[j + 1];
                arr[j + 1] = temp;
            }
        }
    }

    // Parallelize Even Phase
    #pragma omp parallel for private(temp)
    for (int i = 0; i < n / 2; i++) {
        for (int j = i; j < n - i - 1; j += 2) {
            if (arr[j] > arr[j + 1]) {
                temp = arr[j];
                arr[j] = arr[j + 1];
                arr[j + 1] = temp;
            }
        }
    }
}

void printArray(int arr[], int n) {
    for (int i = 0; i < n; i++) {
        printf("%d ", arr[i]);
    }
    printf("\n");
}

void generateRandomArray(int arr[], int n) {
    for (int i = 0; i < n; i++) {
        arr[i] = rand() % 1000;
    }
}

int main() {
    srand(time(0));

    int n;
    printf("Enter the size of the array: ");
    scanf("%d", &n);

    int *arr1 = (int *)malloc(n * sizeof(int));
    int *arr2 = (int *)malloc(n * sizeof(int));
    int *arr3 = (int *)malloc(n * sizeof(int));

    // Generate random numbers for the array
    generateRandomArray(arr1, n);
    for (int i = 0; i < n; i++) {
        arr2[i] = arr1[i];
        arr3[i] = arr1[i];
    }
    printf("Prathmesh Kulkarni 41036 BE A");
    printf("\n+----------------------+------------------------+------------------------+------------------+------------------+\n");
    printf("| Iteration | Odd-Even Bubble Sort Seq Time | Odd-Even Bubble Sort Par Time | Merge Sort Seq Time | Merge Sort Par Time | Speedup | Efficiency |\n");
    printf("+----------------------+------------------------+------------------------+------------------+------------------+\n");

    // Measure performance of sequential algorithms and parallel algorithms for 5 observations
    for (int i = 0; i < 5; i++) {
        // Sequential Odd-Even Bubble Sort
        double start_time = omp_get_wtime();
        bubbleSortOddEvenParallel(arr1, n);
        double end_time = omp_get_wtime();
        double bubbleSortSeqTime = end_time - start_time;

        // Parallel Odd-Even Bubble Sort
        start_time = omp_get_wtime();
        bubbleSortOddEvenParallel(arr1, n);
        end_time = omp_get_wtime();
        double bubbleSortParTime = end_time - start_time;

        // Sequential Merge Sort or Insertion Sort based on size
        start_time = omp_get_wtime();
        if (n < 20) {
            insertionSort(arr2, n);  // Use Insertion Sort if n is less than 20
        } else {
            mergeSortSequential(arr2, 0, n - 1);  // Otherwise, use Merge Sort
        }
        end_time = omp_get_wtime();
        double mergeSortSeqTime = end_time - start_time;

        // Parallel Merge Sort or Insertion Sort based on size
        start_time = omp_get_wtime();
        if (n < 20) {
            insertionSort(arr3, n);  // Use Insertion Sort if n is less than 20
        } else {
            mergeSortParallel(arr3, 0, n - 1);  // Otherwise, use Parallel Merge Sort
        }
        end_time = omp_get_wtime();
        double mergeSortParTime = end_time - start_time;

        // Calculate Speedup and Efficiency
        double bubbleSortSpeedup = bubbleSortSeqTime / bubbleSortParTime;
        double mergeSortSpeedup = mergeSortSeqTime / mergeSortParTime;
        double efficiency = (bubbleSortSpeedup + mergeSortSpeedup) / 6;

        // Print results in table format
        printf("| %9d | %22f | %22f | %16f | %16f | %8f | %10f |\n", 
               i + 1, bubbleSortSeqTime, bubbleSortParTime, mergeSortSeqTime, mergeSortParTime, 
               bubbleSortSpeedup + mergeSortSpeedup, efficiency);
    }

    printf("+----------------------+------------------------+------------------------+------------------+------------------+\n");

    free(arr1);
    free(arr2);
    free(arr3);

    return 0;
}



==================================================================================



										BFS
#include <stdio.h>
#include <stdlib.h>
#include <stdbool.h>
#include <omp.h>
#include <time.h>

typedef struct Node {
    int vertex;
    struct Node* next;
} Node;

Node** createGraph(int n) {
    Node** adjList = (Node**)malloc(n * sizeof(Node*));
    for (int i = 0; i < n; i++) adjList[i] = NULL;
    return adjList;
}

void addEdge(Node** adjList, int u, int v) {
    Node* newNode = (Node*)malloc(sizeof(Node));
    newNode->vertex = v;
    newNode->next = adjList[u];
    adjList[u] = newNode;

    newNode = (Node*)malloc(sizeof(Node));
    newNode->vertex = u;
    newNode->next = adjList[v];
    adjList[v] = newNode;
}

void generateRandomGraph(Node** adjList, int n) {
    srand(time(NULL));

    // Build a tree (guaranteed connected)
    for (int i = 1; i < n; i++) {
        int v = rand() % i;
        addEdge(adjList, i, v);
    }

    // Add more edges
    int extraEdges = n; // can be increased for denser graph
    for (int i = 0; i < extraEdges; i++) {
        int u = rand() % n;
        int v = rand() % n;
        if (u != v) addEdge(adjList, u, v);
    }
}

void bfsSequential(Node** adjList, int n, int start) {
    bool* visited = (bool*)calloc(n, sizeof(bool));
    int* queue = (int*)malloc(n * sizeof(int));
    int front = 0, rear = 0;

    visited[start] = true;
    queue[rear++] = start;

    while (front < rear) {
        int u = queue[front++];
        for (Node* temp = adjList[u]; temp; temp = temp->next) {
            int v = temp->vertex;
            if (!visited[v]) {
                visited[v] = true;
                queue[rear++] = v;
            }
        }
    }

    free(visited);
    free(queue);
}

void bfsParallel(Node** adjList, int n, int start) {
    bool* visited = (bool*)calloc(n, sizeof(bool));
    int* frontier = (int*)malloc(n * sizeof(int));
    int frontierSize = 0;

    visited[start] = true;
    frontier[frontierSize++] = start;

    while (frontierSize > 0) {
        int* nextFrontier = (int*)malloc(n * sizeof(int));
        int nextSize = 0;

        #pragma omp parallel
        {
            int* localQueue = (int*)malloc(n * sizeof(int));
            int localSize = 0;

            #pragma omp for nowait
            for (int i = 0; i < frontierSize; i++) {
                int u = frontier[i];
                for (Node* temp = adjList[u]; temp; temp = temp->next) {
                    int v = temp->vertex;
                    if (!visited[v]) {
                        #pragma omp critical
                        {
                            if (!visited[v]) {
                                visited[v] = true;
                                localQueue[localSize++] = v;
                            }
                        }
                    }
                }
            }

            #pragma omp critical
            {
                for (int i = 0; i < localSize; i++)
                    nextFrontier[nextSize++] = localQueue[i];
            }

            free(localQueue);
        }

        frontierSize = nextSize;
        for (int i = 0; i < nextSize; i++)
            frontier[i] = nextFrontier[i];

        free(nextFrontier);
    }

    free(visited);
    free(frontier);
}

int main() {
    int n;
    printf("Enter number of vertices: ");
    scanf("%d", &n);

    Node** adjList = createGraph(n);
    generateRandomGraph(adjList, n);

    double start, end;

    start = omp_get_wtime();
    bfsSequential(adjList, n, 0);
    end = omp_get_wtime();
    double time_seq = end - start;
    printf("Sequential BFS Time: %.6f s\n", time_seq);

    start = omp_get_wtime();
    bfsParallel(adjList, n, 0);
    end = omp_get_wtime();
    double time_par = end - start;
    printf("Parallel BFS Time: %.6f s\n", time_par);
    printf("BFS Speedup: %.2fx\n", time_seq / time_par);

    // Free memory
    for (int i = 0; i < n; i++) {
        Node* curr = adjList[i];
        while (curr) {
            Node* next = curr->next;
            free(curr);
            curr = next;
        }
    }
    free(adjList);

    return 0;
}
  ================================================================================


									DFS	


#include <stdio.h>
#include <stdlib.h>
#include <stdbool.h>
#include <time.h>
#include <omp.h>

#define MAX_THREADS 8

typedef struct Node {
    int vertex;
    struct Node* next;
} Node;

typedef struct {
    Node** adjList;
    int V;
} Graph;

Graph* createGraph(int V) {
    Graph* graph = (Graph*)malloc(sizeof(Graph));
    graph->V = V;
    graph->adjList = (Node**)malloc(V * sizeof(Node*));
    for (int i = 0; i < V; i++) graph->adjList[i] = NULL;
    return graph;
}

void addEdge(Graph* graph, int src, int dest) {
    Node* newNode = (Node*)malloc(sizeof(Node));
    newNode->vertex = dest;
    newNode->next = graph->adjList[src];
    graph->adjList[src] = newNode;

    newNode = (Node*)malloc(sizeof(Node));
    newNode->vertex = src;
    newNode->next = graph->adjList[dest];
    graph->adjList[dest] = newNode;
}

void generateRandomTree(Graph* graph, int V) {
    for (int i = 1; i < V; i++) {
        int parent = rand() % i;
        addEdge(graph, parent, i);
    }
}

void DFSSequential(Graph* graph, int start, bool* visited) {
    int* stack = (int*)malloc(graph->V * sizeof(int));
    int top = -1;
    stack[++top] = start;

    while (top >= 0) {
        int curr = stack[top--];
        if (!visited[curr]) {
            visited[curr] = true;
            Node* temp = graph->adjList[curr];
            while (temp) {
                if (!visited[temp->vertex])
                    stack[++top] = temp->vertex;
                temp = temp->next;
            }
        }
    }
    free(stack);
}

void DFSParallel(Graph* graph, int start, bool* visited) {
    int* frontier = (int*)malloc(graph->V * sizeof(int));
    int frontierSize = 1;
    frontier[0] = start;
    omp_lock_t* locks = (omp_lock_t*)malloc(graph->V * sizeof(omp_lock_t));
    for (int i = 0; i < graph->V; i++) omp_init_lock(&locks[i]);

    #pragma omp parallel for schedule(dynamic, 1) num_threads(MAX_THREADS)
    for (int i = 0; i < frontierSize; i++) {
        int* localStack = (int*)malloc(graph->V * sizeof(int));
        int top = -1;
        localStack[++top] = frontier[i];

        while (top >= 0) {
            int curr = localStack[top--];

            if (!visited[curr]) {
                omp_set_lock(&locks[curr]);
                if (!visited[curr]) {
                    visited[curr] = true;
                    Node* temp = graph->adjList[curr];
                    while (temp) {
                        if (!visited[temp->vertex])
                            localStack[++top] = temp->vertex;
                        temp = temp->next;
                    }
                }
                omp_unset_lock(&locks[curr]);
            }
        }
        free(localStack);
    }
    for (int i = 0; i < graph->V; i++) omp_destroy_lock(&locks[i]);
    free(locks);
    free(frontier);
}

int main() {
    int V;
    printf("Enter number of vertices: ");
    scanf("%d", &V);

    Graph* graph = createGraph(V);
    generateRandomTree(graph, V);

    bool* visitedSeq = (bool*)calloc(V, sizeof(bool));
    bool* visitedPar = (bool*)calloc(V, sizeof(bool));

    double startTime, endTime;

    startTime = omp_get_wtime();
    DFSSequential(graph, 0, visitedSeq);
    endTime = omp_get_wtime();
    double seqTime = endTime - startTime;
    printf("Sequential DFS Time: %f seconds\n", seqTime);

    startTime = omp_get_wtime();
    DFSParallel(graph, 0, visitedPar);
    endTime = omp_get_wtime();
    double parTime = endTime - startTime;
    printf("Parallel DFS Time:   %f seconds\n", parTime);

    if (parTime > 0)
        printf("Speedup: %.2fx\n", seqTime / parTime);

    free(visitedSeq);
    free(visitedPar);
    for (int i = 0; i < V; i++) {
        Node* temp = graph->adjList[i];
        while (temp) {
            Node* toFree = temp;
            temp = temp->next;
            free(toFree);
        }
    }
    free(graph->adjList);
    free(graph);
    return 0;
}



=======================================================================================


								PARALLEL REDUCTION

#include <iostream>
#include <vector>
#include <chrono>
#include <cuda_runtime.h>
#include <iomanip>
#include <limits>
#include <cstdlib>

#define CUDA_CORES 768  // GTX 1050 Ti CUDA cores
#define BLOCK_SIZE 256  // Optimal block size
#define WARP_SIZE 32

#define CUDA_CHECK(call) \
    { \
        cudaError_t err = call; \
        if (err != cudaSuccess) { \
            std::cerr << "CUDA Error: " << cudaGetErrorString(err) << " at line " << __LINE__ << std::endl; \
            exit(EXIT_FAILURE); \
        } \
    }

// ---------------- Warp-level reduction for SUM ----------------
__inline__ __device__ int warpReduceSum(int val) {
    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)
        val += __shfl_down_sync(0xFFFFFFFF, val, offset);
    return val;
}

// ---------------- Warp-level reduction for MIN ----------------
__inline__ __device__ int warpReduceMin(int val) {
    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)
        val = min(val, __shfl_down_sync(0xFFFFFFFF, val, offset));
    return val;
}

// ---------------- Warp-level reduction for MAX ----------------
__inline__ __device__ int warpReduceMax(int val) {
    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)
        val = max(val, __shfl_down_sync(0xFFFFFFFF, val, offset));
    return val;
}

// ---------------- GPU SUM Kernel ----------------
__global__ void reduceSum(int* input, unsigned long long* output, int n) {
    __shared__ int shared[BLOCK_SIZE];

    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    int val = (tid < n) ? input[tid] : 0;

    val = warpReduceSum(val);

    int lane = threadIdx.x % WARP_SIZE;
    int warpId = threadIdx.x / WARP_SIZE;
    if (lane == 0) shared[warpId] = val;
    __syncthreads();

    if (warpId == 0) {
        val = (lane < blockDim.x / WARP_SIZE) ? shared[lane] : 0;
        val = warpReduceSum(val);
    }

    if (threadIdx.x == 0) atomicAdd(output, (unsigned long long)val);
}

// ---------------- GPU MIN Kernel ----------------
__global__ void reduceMin(int* input, int* output, int n) {
    __shared__ int shared[BLOCK_SIZE];

    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    int val = (tid < n) ? input[tid] : INT_MAX;

    val = warpReduceMin(val);

    int lane = threadIdx.x % WARP_SIZE;
    int warpId = threadIdx.x / WARP_SIZE;
    if (lane == 0) shared[warpId] = val;
    __syncthreads();

    if (warpId == 0) {
        val = (lane < blockDim.x / WARP_SIZE) ? shared[lane] : INT_MAX;
        val = warpReduceMin(val);
    }

    if (threadIdx.x == 0) atomicMin(output, val);
}

// ---------------- GPU MAX Kernel ----------------
__global__ void reduceMax(int* input, int* output, int n) {
    __shared__ int shared[BLOCK_SIZE];

    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    int val = (tid < n) ? input[tid] : INT_MIN;

    val = warpReduceMax(val);

    int lane = threadIdx.x % WARP_SIZE;
    int warpId = threadIdx.x / WARP_SIZE;
    if (lane == 0) shared[warpId] = val;
    __syncthreads();

    if (warpId == 0) {
        val = (lane < blockDim.x / WARP_SIZE) ? shared[lane] : INT_MIN;
        val = warpReduceMax(val);
    }

    if (threadIdx.x == 0) atomicMax(output, val);
}

// ---------------- CPU Functions ----------------
long long sequentialSum(const std::vector<int>& data) {
    long long sum = 0;
    for (int val : data) sum += val;
    return sum;
}

int sequentialMin(const std::vector<int>& data) {
    int minVal = std::numeric_limits<int>::max();
    for (int val : data) minVal = std::min(minVal, val);
    return minVal;
}

int sequentialMax(const std::vector<int>& data) {
    int maxVal = std::numeric_limits<int>::min();
    for (int val : data) maxVal = std::max(maxVal, val);
    return maxVal;
}

double sequentialAverage(const std::vector<int>& data) {
    return static_cast<double>(sequentialSum(data)) / data.size();
}

// ---------------- MAIN ----------------
int main() {
    std::vector<long long> sizes = {55000, 65000, 75000, 85000};
    std::vector<int> maxValues = {1200, 2200, 3200, 4200};

 std::cout << "\n";
 std::cout << "Name: no: CLASS: BE \n";
 std::cout << "\n";
 std::cout << "\n";
    std::cout << "----------------------------------------------------------------------------------------------------------------------------------------------------------\n";
    std::cout << "| Input Size | Max Value | CPU Sum     | GPU Sum     | CPU Time (s) | GPU Time (s) | Speedup | Efficiency | CPU Min | GPU Min | CPU Max | GPU Max | CPU Avg | GPU Avg |\n";
    std::cout << "----------------------------------------------------------------------------------------------------------------------------------------------------------\n";

    for (size_t i = 0; i < sizes.size(); i++) {
        long long n = sizes[i];
        int maxVal = maxValues[i];
        std::vector<int> data(n);

        for (long long j = 0; j < n; ++j)
            data[j] = rand() % maxVal;

        int* d_input;
        unsigned long long* d_sum;
        int* d_min;
        int* d_max;
        unsigned long long h_sum = 0;
        int h_min = INT_MAX;
        int h_max = INT_MIN;

        CUDA_CHECK(cudaMalloc(&d_input, n * sizeof(int)));
        CUDA_CHECK(cudaMalloc(&d_sum, sizeof(unsigned long long)));
        CUDA_CHECK(cudaMalloc(&d_min, sizeof(int)));
        CUDA_CHECK(cudaMalloc(&d_max, sizeof(int)));

        CUDA_CHECK(cudaMemcpy(d_input, data.data(), n * sizeof(int), cudaMemcpyHostToDevice));
        CUDA_CHECK(cudaMemset(d_sum, 0, sizeof(unsigned long long)));
        CUDA_CHECK(cudaMemcpy(d_min, &h_min, sizeof(int), cudaMemcpyHostToDevice));
        CUDA_CHECK(cudaMemcpy(d_max, &h_max, sizeof(int), cudaMemcpyHostToDevice));

        int numBlocks = (n + BLOCK_SIZE - 1) / BLOCK_SIZE;

        // CPU
        auto cpuStart = std::chrono::high_resolution_clock::now();
        long long cpuSum = sequentialSum(data);
        int cpuMin = sequentialMin(data);
        int cpuMax = sequentialMax(data);
        double cpuAvg = sequentialAverage(data);
        auto cpuEnd = std::chrono::high_resolution_clock::now();
        std::chrono::duration<double> cpuTime = cpuEnd - cpuStart;

        // GPU
        cudaEvent_t start, stop;
        float gpuTimeMs = 0.0;
        cudaEventCreate(&start);
        cudaEventCreate(&stop);
        cudaEventRecord(start);

        reduceSum<<<numBlocks, BLOCK_SIZE>>>(d_input, d_sum, n);
        reduceMin<<<numBlocks, BLOCK_SIZE>>>(d_input, d_min, n);
        reduceMax<<<numBlocks, BLOCK_SIZE>>>(d_input, d_max, n);

        cudaEventRecord(stop);
        cudaEventSynchronize(stop);
        cudaEventElapsedTime(&gpuTimeMs, start, stop);

        CUDA_CHECK(cudaMemcpy(&h_sum, d_sum, sizeof(unsigned long long), cudaMemcpyDeviceToHost));
        CUDA_CHECK(cudaMemcpy(&h_min, d_min, sizeof(int), cudaMemcpyDeviceToHost));
        CUDA_CHECK(cudaMemcpy(&h_max, d_max, sizeof(int), cudaMemcpyDeviceToHost));

        double gpuAvg = static_cast<double>(h_sum) / n;
        double speedup = cpuTime.count() / (gpuTimeMs / 1000.0);
        double efficiency = speedup / CUDA_CORES;

        std::cout << "| " << std::setw(10) << n
                  << " | " << std::setw(9) << maxVal
                  << " | " << std::setw(11) << cpuSum
                  << " | " << std::setw(11) << h_sum
                  << " | " << std::setw(12) << std::fixed << std::setprecision(6) << cpuTime.count()
                  << " | " << std::setw(12) << gpuTimeMs / 1000.0
                  << " | " << std::setw(7) << std::fixed << std::setprecision(2) << speedup
                  << " | " << std::setw(10) << std::fixed << std::setprecision(6) << efficiency
                  << " | " << std::setw(8) << cpuMin
                  << " | " << std::setw(8) << h_min
                  << " | " << std::setw(8) << cpuMax
                  << " | " << std::setw(8) << h_max
                  << " | " << std::setw(8) << std::fixed << std::setprecision(2) << cpuAvg
                  << " | " << std::setw(8) << std::fixed << std::setprecision(2) << gpuAvg
                  << " |\n";

        // Cleanup
        cudaFree(d_input);
        cudaFree(d_sum);
        cudaFree(d_min);
        cudaFree(d_max);
    }

    std::cout << "----------------------------------------------------------------------------------------------------------------------------------------------------------\n";
    return 0;
}
 


=======================================================================================

						AADDITION
#include <iostream>
#include <vector>
#include <chrono>
#include <cuda_runtime.h>
#include <iomanip>
#include <fstream>
#include <limits>

#define CUDA_CORES 768  // GTX 1050 Ti CUDA cores
#define BLOCK_SIZE 256  // Best for 1050 Ti
#define WARP_SIZE 32

#define CUDA_CHECK(call) \
    { \
        cudaError_t err = call; \
        if (err != cudaSuccess) { \
            std::cerr << "CUDA Error: " << cudaGetErrorString(err) << " at " << __LINE__ << std::endl; \
            exit(EXIT_FAILURE); \
        } \
    }

// Warp reduction function
__inline__ __device__ int warpReduceSum(int val) {
    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)
        val += __shfl_down_sync(0xFFFFFFFF, val, offset);
    return val;
}

// Hierarchical block-level reduction using shared memory
__global__ void reduceSum(int* input, int* output, int n) {
    __shared__ int sharedData[BLOCK_SIZE]; 

    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    int localSum = (tid < n) ? input[tid] : 0;
    
    // Perform warp-level reduction
    localSum = warpReduceSum(localSum);

    // Store warp results in shared memory
    int lane = threadIdx.x % WARP_SIZE;
    int warpId = threadIdx.x / WARP_SIZE;

    if (lane == 0) sharedData[warpId] = localSum;
    __syncthreads();

    // Reduce shared memory
    if (warpId == 0) {
        localSum = (lane < (blockDim.x / WARP_SIZE)) ? sharedData[lane] : 0;
        localSum = warpReduceSum(localSum);
    }

    // First thread writes block result
    if (threadIdx.x == 0) atomicAdd(output, localSum);
}

// Sequential CPU Sum
long long sequentialSum(const std::vector<int>& data) {
    long long sum = 0;
    for (int val : data) sum += val;
    return sum;
}

// Sequential CPU Min
int sequentialMin(const std::vector<int>& data) {
    int minVal = std::numeric_limits<int>::max();
    for (int val : data) minVal = std::min(minVal, val);
    return minVal;
}

// Sequential CPU Average
double sequentialAverage(const std::vector<int>& data) {
    return static_cast<double>(sequentialSum(data)) / data.size();
}

int main() {
    std::vector<long long> sizes = {95000, 520000, 930000, 1110000,4600000 };
    std::vector<int> maxValues = {1200, 2400, 4600, 5800};
    std::cout << "\n";
 std::cout << "\n";
 std::cout << "Name:ABC no:123 CLASS: BE \n";
 std::cout << "\n";
 std::cout << "\n";
    std::cout << "---------------------------------------------------------------------------------------------------\n";
    std::cout << "| Input Size | Max Value | CPU Sum       | GPU Sum       | CPU Time (s) | GPU Time (s) | Speedup  | Efficiency | Avg Value | Min Value |\n";
    std::cout << "---------------------------------------------------------------------------------------------------\n";

    for (size_t i = 0; i < sizes.size(); i++) {
        long long n = sizes[i];
        int maxValue = maxValues[i];

        std::vector<int> data(n);
        for (long long j = 0; j < n; ++j) {
            data[j] = rand() % maxValue;
        }

        int numBlocks = (n + BLOCK_SIZE - 1) / BLOCK_SIZE;

        int* d_input;
        int* d_output;
        int h_output = 0;

        CUDA_CHECK(cudaMalloc(&d_input, n * sizeof(int)));
        CUDA_CHECK(cudaMalloc(&d_output, sizeof(int)));
        CUDA_CHECK(cudaMemcpy(d_input, data.data(), n * sizeof(int), cudaMemcpyHostToDevice));
        CUDA_CHECK(cudaMemset(d_output, 0, sizeof(int)));

        // CPU Calculation
        auto startSeq = std::chrono::high_resolution_clock::now();
        long long seqSum = sequentialSum(data);
        int seqMin = sequentialMin(data);
        double seqAvg = sequentialAverage(data);
        auto endSeq = std::chrono::high_resolution_clock::now();
        std::chrono::duration<double> seqTime = endSeq - startSeq;

        // GPU Calculation
        float gpuSumTime;
        cudaEvent_t start, end;
        cudaEventCreate(&start);
        cudaEventCreate(&end);

        cudaEventRecord(start);
        reduceSum<<<numBlocks, BLOCK_SIZE>>>(d_input, d_output, n);
        cudaEventRecord(end);
        cudaEventSynchronize(end);
        cudaEventElapsedTime(&gpuSumTime, start, end);

        CUDA_CHECK(cudaMemcpy(&h_output, d_output, sizeof(int), cudaMemcpyDeviceToHost));

        // Speedup Calculation
        double speedup = seqTime.count() / (gpuSumTime / 1000.0);

        // Efficiency Calculation
        double efficiency = speedup / CUDA_CORES;

        // Print Results in Tabular Format
        std::cout << "| " << std::setw(10) << n
                  << " | " << std::setw(9) << maxValue
                  << " | " << std::setw(12) << seqSum
                  << " | " << std::setw(12) << h_output
                  << " | " << std::setw(11) << std::fixed << std::setprecision(6) << seqTime.count()
                  << " | " << std::setw(11) << gpuSumTime / 1000.0
                  << " | " << std::setw(7) << std::fixed << std::setprecision(2) << speedup
                  << " | " << std::setw(10) << std::fixed << std::setprecision(6) << efficiency
                  << " | " << std::setw(9) << std::fixed << std::setprecision(2) << seqAvg
                  << " | " << std::setw(9) << seqMin
                  << " |\n";

        cudaFree(d_input);
        cudaFree(d_output);
    }

    std::cout << "---------------------------------------------------------------------------------------------------\n";
    return 0;
}
   

===================================================================================


								MULTIPLICATION


#include <iostream>
#include <vector>
#include <chrono>
#include <cuda_runtime.h>
#include <iomanip>

#define CUDA_CORES 768  // GTX 1050Ti CUDA cores

#define CUDA_CHECK(call) \
    { \
        cudaError_t err = call; \
        if (err != cudaSuccess) { \
            std::cerr << "CUDA Error: " << cudaGetErrorString(err) << " at " << __LINE__ << std::endl; \
            exit(EXIT_FAILURE); \
        } \
    }

/********************** CUDA Kernel: Matrix Multiplication **********************/
__global__ void matrixMulCanon(int* A, int* B, int* C, int N) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < N && col < N) {
        int sum = 0;
        for (int k = 0; k < N; ++k) {
            sum += A[row * N + k] * B[k * N + col];
        }
        C[row * N + col] = sum;
    }
}

/********************** Sequential Matrix Multiplication **********************/
void sequentialMatrixMul(const std::vector<int>& A, const std::vector<int>& B, std::vector<int>& C, int N) {
    for (int i = 0; i < N; ++i) {
        for (int j = 0; j < N; ++j) {
            int sum = 0;
            for (int k = 0; k < N; ++k) {
                sum += A[i * N + k] * B[k * N + j];
            }
            C[i * N + j] = sum;
        }
    }
}

/********************** Run Matrix Multiplication Test **********************/
void runMatrixMultiplication(int N) {
    std::vector<int> A(N * N), B(N * N), C(N * N);

    for (int i = 0; i < N * N; ++i) {
        A[i] = rand() % 10;
        B[i] = rand() % 10;
    }

    int *d_A, *d_B, *d_C;
    CUDA_CHECK(cudaMalloc(&d_A, N * N * sizeof(int)));
    CUDA_CHECK(cudaMalloc(&d_B, N * N * sizeof(int)));
    CUDA_CHECK(cudaMalloc(&d_C, N * N * sizeof(int)));

    cudaMemcpy(d_A, A.data(), N * N * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, B.data(), N * N * sizeof(int), cudaMemcpyHostToDevice);

    // CPU execution time measurement
    auto startSeq = std::chrono::high_resolution_clock::now();
    sequentialMatrixMul(A, B, C, N);
    auto endSeq = std::chrono::high_resolution_clock::now();
    std::chrono::duration<double> seqTime = endSeq - startSeq;

    // GPU execution time measurement
    float gpuTime;
    cudaEvent_t start, end;
    cudaEventCreate(&start);
    cudaEventCreate(&end);

    dim3 blockDim(16, 16);
    dim3 gridDim((N + 15) / 16, (N + 15) / 16);

    cudaEventRecord(start);
    matrixMulCanon<<<gridDim, blockDim>>>(d_A, d_B, d_C, N);
    cudaEventRecord(end);
    cudaEventSynchronize(end);
    cudaEventElapsedTime(&gpuTime, start, end);

    // Check for CUDA errors after kernel execution
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        std::cerr << "CUDA Kernel Error: " << cudaGetErrorString(err) << std::endl;
        exit(EXIT_FAILURE);
    }

    double speedup = seqTime.count() / (gpuTime / 1000.0);
    double efficiency = speedup / CUDA_CORES;

    std::cout << std::setw(15) << N 
              << std::setw(20) << seqTime.count() 
              << std::setw(20) << gpuTime 
              << std::setw(20) << speedup 
              << std::setw(20) << efficiency 
              << std::endl;

    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);
}

/********************** Main Function **********************/
int main() {
std::cout << "\n";
   std::cout << "\n";
 std::cout << "Name: no: CLASS: BE \n";
 std::cout << "\n";
 std::cout << "\n";
    std::cout << "\n==== Matrix Multiplication Tests ====\n";
    std::cout << std::setw(15) << "Matrix Size" 
              << std::setw(20) << "CPU Time (s)" 
              << std::setw(20) << "GPU Time (ms)" 
              << std::setw(20) << "Speedup" 
              << std::setw(20) << "Efficiency" 
              << std::endl;
    std::cout << std::string(95, '-') << "\n";

    int matrixTestCases[] = {250,600, 1700, 2800};  // Large matrix sizes
    for (int i = 0; i < 4; i++) {
        runMatrixMultiplication(matrixTestCases[i]);
    }

    return 0;
}


